# spark://127.0.0.1:7077 or local
# local[n] is a special value that runs Spark on n threads on the local machine,
# without connecting to a cluster.
spark.master=local[2]

# Spark application name.
spark.application-name=Machine Learning with Apache Spark

# Path to distributed library that should be loaded into each worker of a Spark cluster.
spark.distributed-libraries=/Users/senthuran/devs/spark/spark-2.4.5-bin-hadoop2.7

# Amount of memory to use for the driver process.
spark.driver.memory=2g

# The maximum amount of CPU cores to request for the application from across the cluster (not from each machine).
spark.cores.max=4

# Amount of memory to assign to each executor process
spark.executor.memory=4g

# The largest number of partitions in a parent RDD during distributed shuffle operations.
# For local mode should be equal to number of cores on the local machine.
spark.default.parallelism=4

# Serializer: org.apache.spark.serializer.JavaSerializer (default) or org.apache.spark.serializer.KryoSerializer
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.kryoserializer.buffer.max=128m

# The number of partitions to use when shuffling data for joins or aggregations.
spark.sql.shuffle.partitions=5